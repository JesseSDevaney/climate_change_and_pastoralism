{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = \"../surveys/plain_txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\"leche\", \"manteca\", \"quesillo\", \"queso\", \"dulce de leche\",\n",
    "                \"cuero curtido\", \"cuero crudo\", \"lana vellón\", \"lana hilo\",\n",
    "                \"lana descerdada\", \"pelo mohair\", \"pelo de caballo\",\n",
    "                \"artesanías textiles\", \"charqui\", \"chalona\",\n",
    "                \"embutidos \\/ chacinados\", \"carne de ovino\",\n",
    "                \"carne de cabra\", \"carne de bovino\", \"carne de cerdo\",\n",
    "                \"carne de pollo\", \"carne de equinos\", \"carne de camélidos\",\n",
    "                \"guano\", \"otros\"]\n",
    "translation_list = [\"milk\", \"butter\", \"quesillo\", \"cheese\", \"caramel\",\n",
    "                    \"tanned_leather\", \"rawhide\", \"fleece_wool\", \"wool_yarn\",\n",
    "                    \"uncerned wool\", \"mohair\", \"horsehair\",\n",
    "                    \"textile_handicrafts\", \"jerky\", \"dried_meat\", \"sausages\",\n",
    "                    \"sheep_meat\", \"goat_meat\", \"beef\", \"pork_meat\",\n",
    "                    \"chicken_meat\", \"equine_meat\", \"camelid_meat\", \"guano\"]\n",
    "# column headers not including product column\n",
    "column_headers = [\"has_produced\", \"quantity\", \"unit_of_measure\",\n",
    "                  \"periodicity_1\", \"number_of_periods_1\",\n",
    "                  \"periodicity_2\", \"number_of_periods_2\",\n",
    "                  \"self_consume\", \"commericial\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between(str_1, str_2, contents):\n",
    "    regex_between = str_1 + r\"([\\w\\W]*)\" + str_2\n",
    "    regex_search = re.findall(regex_between, contents)\n",
    "\n",
    "    return regex_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interviewee(contents):\n",
    "    name_regex = r\"nombre de la persona[^\\n:]*:[\\s]*([^\\n]*)\\n\"\n",
    "    name_search = re.findall(name_regex, contents)\n",
    "    if len(name_search) == 1:\n",
    "        name = name_search[0]\n",
    "        if name == \"3\":\n",
    "            name = \"N/A\"\n",
    "    else:\n",
    "        alt_name_regex = r\"nombre de la persona[^\\n\\(]*\\(\\w* +([^\\n]*)\\n\"\n",
    "        alt_name_search = re.findall(alt_name_regex, contents)\n",
    "        if len(alt_name_search) == 1:\n",
    "            name = alt_name_search[0]\n",
    "            if name == \"3\":\n",
    "                name = \"N/A\"\n",
    "        else:\n",
    "            name = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_date_range(section_search):\n",
    "    date_range = \"\"\n",
    "\n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0]\n",
    "\n",
    "        date_regex = r\"([\\w ]+\\d+ *[-–] *[\\w ]+\\d+)\"\n",
    "        date_search = re.findall(date_regex, section_string)\n",
    "\n",
    "        if len(date_search) == 1:\n",
    "            date_range = date_search[0]\n",
    "        else:\n",
    "            date_range = \"UNENCOUNTERED FORMAT\"\n",
    "    else:\n",
    "        date_range = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    return date_range\n",
    "\n",
    "\n",
    "def parse_section_4_1(section_search, product_list,\n",
    "                      translation_list, column_headers):\n",
    "    section_4_1_data = []\n",
    "\n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0]\n",
    "\n",
    "        # iterate through products\n",
    "        for i in range(len(product_list)-1):\n",
    "            item_dict = {}\n",
    "            item_1 = product_list[i]\n",
    "            item_2 = product_list[i+1]\n",
    "            term = translation_list[i]\n",
    "\n",
    "            row_search = find_between(item_1, item_2, section_string)\n",
    "\n",
    "            item_dict[\"product\"] = term\n",
    "\n",
    "            # if product row found\n",
    "            if len(row_search) == 1:\n",
    "                row_string = row_search[0]\n",
    "                row_list = row_string.split(\"\\n\")\n",
    "\n",
    "                if len(row_list) == 21:\n",
    "                    # skip odd indexes (1, 3, 5, 7)\n",
    "                    # drop first two rows (start count at -2)\n",
    "                    count = -2\n",
    "                    if (row_list[-1 * count] == \"\"\n",
    "                       and row_list[-1 * count + 2] != \"\"):\n",
    "                        count -= 2\n",
    "\n",
    "                    for j in range(len(row_list)):\n",
    "                        if count > 16:\n",
    "                            break;\n",
    "                        if count >= 0 and count % 2 == 0:\n",
    "                            col = column_headers[count//2]\n",
    "                            entry = row_list[j]\n",
    "                            entry = entry.strip()\n",
    "                            item_dict[col] = entry\n",
    "                        count += 1\n",
    "                else:\n",
    "                    for col in column_headers:\n",
    "                        item_dict[col] = \"UNENCOUNTERED FORMAT\"\n",
    "            else:\n",
    "                for col in column_headers:\n",
    "                    item_dict[col] = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "            section_4_1_data.append(item_dict)\n",
    "    else:\n",
    "        section_4_1_data = [{}]\n",
    "\n",
    "    return section_4_1_data\n",
    "\n",
    "\n",
    "def parse_otros_4_1(otros_search, column_headers):\n",
    "    otros_4_1_data = []\n",
    "\n",
    "    # parsing section for otros #\n",
    "    if len(otros_search) == 1:\n",
    "        otros_string = otros_search[0]\n",
    "        otros_list = otros_string.split(\"\\n\")\n",
    "        otros_list = otros_list[1:]\n",
    "\n",
    "        if len(otros_list) > 65:\n",
    "            # skip odd indexes (1, 3, 5, 7)\n",
    "            count = 0\n",
    "            count_restart = -2\n",
    "            count_end = 20\n",
    "\n",
    "            item_dict = {}\n",
    "            for item in otros_list:\n",
    "                if count == count_end:\n",
    "                    count = count_restart\n",
    "                    otros_4_1_data.append(item_dict)\n",
    "                    item_dict = {}\n",
    "                if count == 0:\n",
    "                    stripped_item = item.strip()\n",
    "                    item_dict[\"product\"] = stripped_item\n",
    "                if count > 0 and count % 2 == 0:\n",
    "                    col = column_headers[(count-2)//2]\n",
    "                    item_dict[col] = item\n",
    "                count += 1\n",
    "        else:\n",
    "            otros_4_1_data = [{}]\n",
    "    else:\n",
    "        otros_4_1_data = [{}]\n",
    "\n",
    "    return otros_4_1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Whole File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(source_directory, product_list,\n",
    "                  translation_list, column_headers):\n",
    "    dataset = []\n",
    "\n",
    "    for process_file in os.listdir(source_directory):\n",
    "        file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "        # with statements automatically control the closing of files\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            contents = contents.lower()\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "            interviewee = parse_interviewee(contents)\n",
    "\n",
    "            section_start = \"iv-1\"\n",
    "            section_end = \"iv-2\"\n",
    "            section_search = find_between(section_start, section_end, contents)\n",
    "\n",
    "            otros_start = r\"iv-1[\\w\\W]*otros[^\\n]*\\n\"\n",
    "            otros_search = find_between(otros_start, section_end, contents)\n",
    "\n",
    "            date_range = parse_date_range(section_search)\n",
    "            section_4_1_data = parse_section_4_1(section_search, product_list,\n",
    "                                                 translation_list, column_headers)\n",
    "            otros_4_1_data = parse_otros_4_1(otros_search, column_headers)\n",
    "\n",
    "            # check to make sure files contain the right amount of entries\n",
    "            if len(section_4_1_data) != 24:\n",
    "                print(f\"Unencountered Format: {process_file}\\n\"\n",
    "                      + f\"Length of section_data: {len(section_4_1_data)}\\n\")\n",
    "\n",
    "            if len(otros_4_1_data) != 3 and len(otros_4_1_data) != 4:\n",
    "                print(f\"Unencountered Format: {process_file}\\n\"\n",
    "                      + f\"Length of otros_data: {len(otros_4_1_data)}\\n\")\n",
    "\n",
    "            for row in section_4_1_data:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = interviewee\n",
    "                data_dict[\"date_range\"] = date_range\n",
    "\n",
    "                for key, value in row.items():\n",
    "                    data_dict[key] = value\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "            for row in otros_4_1_data:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = interviewee\n",
    "                data_dict[\"date_range\"] = date_range\n",
    "\n",
    "                for key, value in row.items():\n",
    "                    data_dict[key] = value\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "    # convert list to DataFrame\n",
    "    raw_df = pd.DataFrame(data=dataset)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_has_produced(dataf):\n",
    "    map_dict = {\"nos\": \"no\",\n",
    "                \"no \": \"no\"}\n",
    "    dataf[\"has_produced\"] = dataf[\"has_produced\"].replace(map_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def standardize_unit_of_measure(dataf):\n",
    "    map_dict = {\"kgs\": \"kg\",\n",
    "                \"k.\": \"kg\",\n",
    "                \"k\": \"kg\",\n",
    "                \"kilos\": \"kg\",\n",
    "                \"kilogramo\": \"kg\",\n",
    "                \"kilogramos\": \"kg\",\n",
    "                \"kig\": \"kg\",\n",
    "                \"kilo\": \"kg\",\n",
    "                \"unidades\": \"unit\",\n",
    "                \"unit\": \"unit\",\n",
    "                \"unidad\": \"unit\",\n",
    "                \"cueros/unidades\": \"unit\",\n",
    "                \"unudades\": \"unit\",\n",
    "                \"animales\": \"unit\",\n",
    "                \"moldes\": \"unit\",\n",
    "                \"molde\": \"unit\",\n",
    "                \"u\": \"unit\",\n",
    "                \"litros\": \"L\",\n",
    "                \"litro\": \"L\",\n",
    "                \"l\": \"L\",\n",
    "                \"lts\": \"L\",\n",
    "                \"lts.\": \"L\",\n",
    "                \"camionada\": \"truck_load\",\n",
    "                \"camionadas\": \"truck_load\",\n",
    "                \"m3\": \"cubic_meters\"}\n",
    "\n",
    "    dataf[\"unit_of_measure\"] = dataf[\"unit_of_measure\"].replace(map_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def transform_date_range(dataf):\n",
    "    replace_dict = {\"agosto 2018 - agosto 2019\": \"august 2018-2019\",\n",
    "                    \"septiembre 2018 - septiembre 2019\": \"september 2018-2019\",\n",
    "                    \"octubre 2018 – octubre 2019\": \"october 2018-2019\",\n",
    "                    \"octubre 2018 - octubre 2019\": \"october 2018-2019\"}\n",
    "    dataf[\"date_range\"] = dataf[\"date_range\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def map_periodicity(dataf):\n",
    "    map_dict = {\"1\": \"day\",\n",
    "                \"2\": \"week\",\n",
    "                \"3\": \"month\",\n",
    "                \"4\": \"year\"}\n",
    "\n",
    "    dataf[\"periodicity_1\"] = dataf[\"periodicity_1\"].replace(map_dict)\n",
    "    dataf[\"periodicity_2\"] = dataf[\"periodicity_2\"].replace(map_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def map_consumption(dataf):\n",
    "    map_dict = {\"0\": 0,\n",
    "                \"1\": 0.25,\n",
    "                \"2\": 0.5,\n",
    "                \"3\": 0.75,\n",
    "                \"4\": 1}\n",
    "\n",
    "    dataf[\"self_consume_fraction\"] = dataf[\"self_consume\"].replace(map_dict)\n",
    "    dataf[\"commericial_fraction\"] = dataf[\"commericial\"].replace(map_dict)\n",
    "\n",
    "    dataf = dataf.drop([\"self_consume\", \"commericial\"], axis=1)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def map_has_produced(dataf):\n",
    "    # convert 'no' values to 'si' if the row contains more than 4 non-null\n",
    "    # values (i.e. someone accidentally put no in that column)\n",
    "    replace_filter = ((dataf[\"has_produced\"] == \"no\")\n",
    "                      & (dataf.isnull().sum(axis=1) < 5))\n",
    "    dataf[\"has_produced\"] = dataf[\"has_produced\"].where(~replace_filter, \"si\")\n",
    "\n",
    "    # convert values to English\n",
    "    map_dict = {\"si\": \"yes\"}\n",
    "    dataf[\"has_produced\"] = dataf[\"has_produced\"].replace(map_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_quantity(dataf):\n",
    "    # decimal separator check\n",
    "    # if quantity value contains a period followed by three digits\n",
    "    # assume that the number entered means thousands (e.g 10.000, 20.000).\n",
    "    # Replace the '.' with an empty character so that when the quantity column\n",
    "    # is converted to float, it does not assume decimal separator\n",
    "    replace_filter = dataf[\"quantity\"].str.contains(\"\\.\\d{3}\", na=False)\n",
    "    replace_series = dataf[\"quantity\"].str.replace(\".\", \"\")\n",
    "    dataf[\"quantity\"] = dataf[\"quantity\"].where(~replace_filter, replace_series)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    return dataf.copy()\n",
    "\n",
    "\n",
    "def handle_null_data(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = ((isnull_sum >= 9)\n",
    "                   | ((isnull_sum == 8) & (dataf[\"has_produced\"] == \"no\")))\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def standardize_columns(dataf):\n",
    "    dataf = standardize_has_produced(dataf)\n",
    "    dataf = standardize_unit_of_measure(dataf)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def transform_values(dataf):\n",
    "    dataf = map_periodicity(dataf)\n",
    "    dataf = map_consumption(dataf)\n",
    "    dataf = map_has_produced(dataf)\n",
    "    dataf = clean_quantity(dataf)\n",
    "    dataf = transform_date_range(dataf)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def set_dtypes(dataf):\n",
    "    dataf[\"quantity\"] = dataf[\"quantity\"].astype(float)\n",
    "\n",
    "    dataf[\"number_of_periods_1\"] = dataf[\"number_of_periods_1\"].astype(float)\n",
    "    dataf[\"number_of_periods_2\"] = dataf[\"number_of_periods_2\"].astype(float)\n",
    "    dataf[\"self_consume_fraction\"] = dataf[\"self_consume_fraction\"].astype(float)\n",
    "    dataf[\"commericial_fraction\"] = dataf[\"commericial_fraction\"].astype(float)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw and Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = process_files(source_directory, product_list,\n",
    "                       translation_list, column_headers)\n",
    "\n",
    "clean_df = (raw_df\n",
    "            .pipe(start_pipeline)\n",
    "            .pipe(standardize_columns)\n",
    "            .pipe(handle_null_data)\n",
    "            .pipe(transform_values)\n",
    "            .pipe(set_dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "### Functions to Check the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_df(raw_dataf, clean_dataf):\n",
    "    raw_indexes = raw_dataf.index\n",
    "    clean_indexes = clean_dataf.index\n",
    "\n",
    "    dropped_indexes = raw_indexes[~raw_indexes.isin(clean_indexes)]\n",
    "    dropped_df = raw_dataf.loc[dropped_indexes].copy()\n",
    "\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropped DataFrame\n",
    "Create the `dropped_df` and collect the rows which contain no non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = get_dropped_df(raw_df, clean_df)\n",
    "\n",
    "# get rows that were dropped but do not contain a has_produced == \"no\"\n",
    "null_df = dropped_df.loc[(dropped_df[\"has_produced\"] != \"no\")\n",
    "                         & (~dropped_df[\"product\"].isin([\"1\", \"2\", \"3\", \"4\"])),\n",
    "                         [\"filename\", \"product\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_row(section_search, product,\n",
    "                    product_list, translation_list):\n",
    "    dropped_row = {}\n",
    "\n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0] + \"v-3:\"\n",
    "\n",
    "        dropped_row[\"product\"] = product\n",
    "\n",
    "        product_index = translation_list.index(product)\n",
    "        item_1 = product_list[product_index]\n",
    "        item_2 = product_list[product_index + 1]\n",
    "\n",
    "        row_search = find_between(item_1, item_2, section_string)\n",
    "\n",
    "        # if product row found\n",
    "        if len(row_search) == 1:\n",
    "            row_string = row_search[0]\n",
    "            row_list = row_string.split(\"\\n\")\n",
    "            row_list_len = len(row_list)\n",
    "\n",
    "            if row_list_len >= 21:\n",
    "                count = -2\n",
    "                for j in range(row_list_len):\n",
    "                    if count > 16:\n",
    "                        break\n",
    "                    if count >= 0:\n",
    "                        col = str(count)\n",
    "                        entry = row_list[j]\n",
    "                        entry = entry.strip()\n",
    "                        dropped_row[col] = entry\n",
    "                    count += 1\n",
    "\n",
    "            else:\n",
    "                for j in range(17):\n",
    "                    dropped_row[str(j)] = \"UNENCOUNTERED FORMAT\"\n",
    "        else:\n",
    "            for j in range(17):\n",
    "                dropped_row[str(j)] = \"UNENCOUNTERED FORMAT\"\n",
    "    else:\n",
    "        dropped_row = {}\n",
    "\n",
    "    return dropped_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dropped_rows(source_directory, product_list, translation_list, s):\n",
    "    process_file = s[\"filename\"]\n",
    "    product = s[\"product\"]\n",
    "\n",
    "    check_dict = {}\n",
    "\n",
    "    file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "    # with statements automatically control the closing of files\n",
    "    with open(file_path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "        contents = contents.lower()\n",
    "\n",
    "        section_start = \"iv-1\"\n",
    "        section_end = \"iv-2\"\n",
    "        section_search = find_between(section_start, section_end, contents)\n",
    "\n",
    "        dropped_row = get_dropped_row(section_search, product,\n",
    "                                      product_list, translation_list)\n",
    "\n",
    "        # check to make sure files contain the right amount of entries\n",
    "        if len(dropped_row) <= 1:\n",
    "            print(f\"Unencountered Format: {process_file}\\n\"\n",
    "                  + f\"Length of data: {len(dropped_row)}\\n\")\n",
    "\n",
    "        check_dict[\"filename\"] = process_file\n",
    "\n",
    "        for key, value in dropped_row.items():\n",
    "            check_dict[key] = value\n",
    "\n",
    "    # convert list to Series\n",
    "    check_series = pd.Series(data=check_dict)\n",
    "\n",
    "    return check_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = null_df.apply(lambda s: check_dropped_rows(source_directory,\n",
    "                                                      product_list,\n",
    "                                                      translation_list,\n",
    "                                                      s),\n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_check(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = isnull_sum >= 17\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df = (check_df\n",
    "               .pipe(start_pipeline)\n",
    "               .pipe(drop_null_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_null_df.shape[0] == 0:\n",
    "    datasets_directory = \"../datasets/\"\n",
    "    filename = \"section_4_1.csv\"\n",
    "    file_path = os.path.join(datasets_directory, filename)\n",
    "\n",
    "    clean_df.to_csv(file_path, index=False, na_rep=\"null\")\n",
    "\n",
    "    print(f\"Exported to {filename}\")\n",
    "else:\n",
    "    print(\"Error: Dropped rows contain non-null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Testing Code\n",
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = clean_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(clean_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows\n",
    "Another way to check is by opening the variable inspector and manually scrolling\n",
    "through the `df_dropped` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tells us whether or not there are files that contain no information for this section. If raw file count is the same as clean file count, then no files were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw File Count: {}\".format(len(raw_df[\"filename\"].value_counts())))\n",
    "print(\"Clean File Count: {}\".format(len(clean_df[\"filename\"].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are files that are completely dropped, find those files and look at them to check for parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = raw_df[\"filename\"].value_counts().index\n",
    "clean_file_names = clean_df[\"filename\"].value_counts().index\n",
    "\n",
    "dropped_files = raw_file_names[~raw_file_names.isin(clean_file_names)].to_list()\n",
    "dropped_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows value counts to see if any rows are being dropped that should not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = dropped_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(dropped_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = clean_df.columns[clean_df.columns != \"filename\"]\n",
    "clean_df[clean_df.loc[:, df_columns].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

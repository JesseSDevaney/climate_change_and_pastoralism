{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = \"../surveys/plain_txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interviewee(contents):\n",
    "    name_regex = r\"nombre de la persona[^\\n:]*:[\\s]*([^\\n]*)\\n\"\n",
    "    name_search = re.findall(name_regex, contents)\n",
    "    if len(name_search) == 1:\n",
    "        name = name_search[0]\n",
    "        if name == \"3\":\n",
    "            name = \"N/A\"\n",
    "    else:\n",
    "        alt_name_regex = r\"nombre de la persona[^\\n\\(]*\\(\\w* +([^\\n]*)\\n\"\n",
    "        alt_name_search = re.findall(alt_name_regex, contents)\n",
    "        if len(alt_name_search) == 1:\n",
    "            name = alt_name_search[0]\n",
    "            if name == \"3\":\n",
    "                name = \"N/A\"\n",
    "        else:\n",
    "            name = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_section_2(contents):\n",
    "    regex_string = r\"2\\.no\\n\\n([\\w\\W]*)sección iii\"\n",
    "    regex_list = re.findall(regex_string, contents)\n",
    "\n",
    "    entry_fields = [\"order_number\", \"relationship_with_reference\",\n",
    "                    \"sex\", \"age\", \"education\", \"employment_situation\",\n",
    "                    \"employment_type\", \"main_activity\",\n",
    "                    \"livestock_owner\"]\n",
    "\n",
    "    entry_list = []\n",
    "    entry = {}\n",
    "\n",
    "    if len(regex_list) == 1:\n",
    "        section_2 = regex_list[0]\n",
    "\n",
    "        if re.search(\"\\n\", section_2):\n",
    "            section_2_list = section_2.split(\"\\n\")\n",
    "\n",
    "            # count until 18, skip odd indexes (1, 3, 5, 7, ...)\n",
    "            count = 0\n",
    "            for item in section_2_list:\n",
    "                item = item.strip()\n",
    "                if count == 18:\n",
    "                    entry_list.append(entry)\n",
    "                    entry = {}\n",
    "                    count = 0\n",
    "                if count % 2 == 0:\n",
    "                    entry_field = entry_fields[count//2]\n",
    "                    entry[entry_field] = item\n",
    "                count += 1\n",
    "        else:\n",
    "            for entry_field in entry_fields:\n",
    "                entry[entry_field] = \"UNENCOUNTERED FORMAT\"\n",
    "            entry_list.append(entry)\n",
    "    else:\n",
    "        for entry_field in entry_fields:\n",
    "            entry[entry_field] = \"UNENCOUNTERED FORMAT\"\n",
    "        entry_list.append(entry)\n",
    "\n",
    "    return entry_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Whole File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(source_directory):\n",
    "    dataset = []\n",
    "\n",
    "    for process_file in os.listdir(source_directory):\n",
    "        file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "        # with statements automatically control the closing of files\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            contents = contents.lower()\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "            section_2_entry_list = parse_section_2(contents)\n",
    "\n",
    "            for entry in section_2_entry_list:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = parse_interviewee(contents)\n",
    "\n",
    "                for key, value in entry.items():\n",
    "                    data_dict[key] = value\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "    # convert list to DataFrame\n",
    "    raw_df = pd.DataFrame(data=dataset)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "### Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_relationship(dataf):\n",
    "    # handle null values\n",
    "    dataf = dataf.replace(\"-\", np.NaN)\n",
    "\n",
    "    # handle relationships with additional info\n",
    "    split_relationship = dataf[\"relationship_with_reference\"].str.split()\n",
    "\n",
    "    rels_with_add_info = split_relationship[split_relationship.str.len() > 1]\n",
    "\n",
    "    relationship = split_relationship.str.get(0)\n",
    "    relationship = relationship.rename(\"relationship_with_reference\")\n",
    "\n",
    "    additional_info = rels_with_add_info.str.slice(start=1).str.join(\" \")\n",
    "    additional_info = additional_info.rename(\"relationship_additional_info\")\n",
    "\n",
    "    # find non-null values for additional info\n",
    "    additional_index = additional_info[~additional_info.isnull()].index\n",
    "\n",
    "    relationship[additional_index] = additional_info[~additional_info.isnull()]\n",
    "\n",
    "    # convert relationship number to its corresponding description\n",
    "    replace_dict = {\n",
    "                             \"1\": \"head/boss\",\n",
    "                             \"2\": \"spouse/concubine\",\n",
    "                             \"3\": \"child/stepchild\",\n",
    "                             \"4\": \"mother/father\",\n",
    "                             \"5\": \"mother-in-law/father-in-law\",\n",
    "                             \"6\": \"sister/brother\",\n",
    "                             \"7\": \"another relative\",\n",
    "                             \"8\": \"other unfamiliar\",\n",
    "                             \"bis nieto\": \"great-grandchild\",\n",
    "                             \"nieto\": \"grandchild\",\n",
    "                             \"nieta\": \"grandchild\",\n",
    "                             \"nuera\": \"child-in-law\",\n",
    "                             \"hijastro\": \"child/stepchild\"\n",
    "                         }\n",
    "\n",
    "    relationship = relationship.replace(replace_dict)\n",
    "\n",
    "    # drop original relationship_with_reference column\n",
    "    dataf = dataf.drop(\"relationship_with_reference\", axis=1)\n",
    "\n",
    "    # merge new relationship_with_reference\n",
    "    dataf = dataf.merge(relationship, how=\"left\", left_index=True,\n",
    "                        right_index=True)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_sex(dataf):\n",
    "    replace_dict = {\"hombre\": \"male\",\n",
    "                    \"mujer\": \"female\",\n",
    "                    \"m\": \"male\",\n",
    "                    \"f\": \"female\",\n",
    "                    \"1\": \"male\",\n",
    "                    \"2\": \"female\"}\n",
    "    dataf[\"sex\"] = dataf[\"sex\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_age(dataf):\n",
    "    dataf[\"age\"] = dataf[\"age\"].replace(\"16-26\", np.NaN)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_education(dataf):\n",
    "    replace_dict = {\n",
    "                       \"1\": \"no school\",\n",
    "                       \"2\": \"primary incomplete\",\n",
    "                       \"3\": \"primary complete\",\n",
    "                       \"4\": \"secondary incomplete\",\n",
    "                       \"5\": \"secondary complete\",\n",
    "                       \"6\": \"tertiary incomplete\",\n",
    "                       \"7\": \"tertiary complete\",\n",
    "                       \"8\": \"university incomplete\",\n",
    "                       \"9\": \"university complete\"\n",
    "                    }\n",
    "\n",
    "    dataf[\"education\"] = dataf[\"education\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_employment_situation(dataf):\n",
    "    replace_dict = {\n",
    "                       \"1\": \"formal employment\",\n",
    "                       \"2\": \"informal employment\",\n",
    "                       \"3\": \"none\",\n",
    "                       \"indep.\": \"independent\",\n",
    "                       \"indepen.\": \"independent\",\n",
    "                       \"jubilada\": \"retired\",\n",
    "                       \"pension.\": \"pension\",\n",
    "                   }\n",
    "\n",
    "    dataf[\"employment_situation\"] = dataf[\"employment_situation\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_employment_type(dataf):\n",
    "    replace_dict = {\n",
    "                       \"1\": \"permanent work\",\n",
    "                       \"1\\t1\": \"permanent work\",\n",
    "                       \"2\": \"temporary work\",\n",
    "                       \"2\\t2\": \"temporary work\",\n",
    "                       \"0\": np.NaN,\n",
    "                       \"_\": np.NaN,\n",
    "                       \"--\\t-\": np.NaN,\n",
    "                       \"-\\t-\": np.NaN\n",
    "                   }\n",
    "\n",
    "    dataf[\"employment_type\"] = dataf[\"employment_type\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_main_activity(dataf):\n",
    "    replace_dict = {\n",
    "                       \"1\": \"none\",\n",
    "                       \"2\": \"agriculture\",\n",
    "                       \"3\": \"livestock\",\n",
    "                       \"4\": \"horticulture\",\n",
    "                       \"5\": \"fishing/hunting\",\n",
    "                       \"6\": \"forestry\",\n",
    "                       \"7\": \"beekeeping\",\n",
    "                       \"8\": \"trade\",\n",
    "                       \"9\": \"craftsmanship\",\n",
    "                       \"10\": \"civil servant\",\n",
    "                       \"11\": \"student\",\n",
    "                       \"12\": \"none\",\n",
    "                       \"13\": \"other\",\n",
    "                       \"3 y 13: turismo)\": \"livestock and tourism\",\n",
    "                       \"3,13- personal de service en establecimiento educativo\": \"livestock and service personnel\",\n",
    "                       \"13 cineasta\": \"filmmaker\",\n",
    "                       \"13 (jubilada)\": \"retired\",\n",
    "                       \"13 (veterin.)\": \"veterin.\",\n",
    "                       \"13(enfermera)\": \"nurse\",\n",
    "                       \"13 médica\": \"doctor\",\n",
    "                       \"13 personal de servicio\": \"service personnel\",\n",
    "                       \"13 bioquimico\": \"biochemical\",\n",
    "                       \"13 albañil\": \"mason\",\n",
    "                       \"13 mecanico\": \"mechanic\",\n",
    "                       \"13 (adminis.)\": \"administrator\",\n",
    "                       \"13 turismo\": \"tourism\",\n",
    "                       \"13 ama de casa\": \"housewife\",\n",
    "                       \"13ama d casa\": \"housewife\",\n",
    "                       \"13 (sicólog)\": \"psychologist\",\n",
    "                       \"3/2\": \"agriculture and livestock\",\n",
    "                       \"2-3\": \"agriculture and livestock\",\n",
    "                       \"2/3\": \"agriculture and livestock\",\n",
    "                       \"2, 3\": \"agriculture and livestock\",\n",
    "                       \"3-9\": \"livestock and trade\",\n",
    "                       \"2, 3, 7\": \"agriculture, livestock, and beekeeping\",\n",
    "                       \"9-3\": \"livestock and craftsmanship\",\n",
    "                       \"3_2\": \"agriculture and livestock\",\n",
    "                       \"2-9\": \"agriculture and craftsmanship\",\n",
    "                       \"3-8\": \"livestock and trade\",\n",
    "                       \"3-\": \"livestock\",\n",
    "                       \"2-3-\": \"agriculture and livestock\",\n",
    "                       \"discapacitado\": \"disabled\"\n",
    "                   }\n",
    "\n",
    "    dataf[\"main_activity\"] = dataf[\"main_activity\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_livestock_keeper(dataf):\n",
    "    replace_dict = {\n",
    "                       \"1\": \"yes\",\n",
    "                       \"2\": \"no\",\n",
    "                       \"si\": \"yes\",\n",
    "                       \"_\": np.NaN,\n",
    "                       \"--\\t-\": np.NaN,\n",
    "                       \"-\\t-\": np.NaN\n",
    "                   }\n",
    "\n",
    "    dataf[\"livestock_owner\"] = dataf[\"livestock_owner\"].replace(replace_dict)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    return dataf.copy() \n",
    "\n",
    "\n",
    "def handle_null_data(dataf):\n",
    "    # drop order number, since it does not contain useful information\n",
    "    dataf = dataf.drop(\"order_number\", axis=1)\n",
    "\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = isnull_sum > 5\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_columns(dataf):\n",
    "    return (dataf\n",
    "            .pipe(clean_relationship)\n",
    "            .pipe(clean_sex)\n",
    "            .pipe(clean_age)\n",
    "            .pipe(clean_education)\n",
    "            .pipe(clean_employment_situation)\n",
    "            .pipe(clean_employment_type)\n",
    "            .pipe(clean_main_activity)\n",
    "            .pipe(clean_livestock_keeper))\n",
    "\n",
    "\n",
    "def set_dtypes(dataf):\n",
    "    dataf[\"age\"] = dataf[\"age\"].astype(float)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw and Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = process_files(source_directory)\n",
    "\n",
    "clean_df = (raw_df\n",
    "            .pipe(start_pipeline)\n",
    "            .pipe(handle_null_data)\n",
    "            .pipe(clean_columns)\n",
    "            .pipe(set_dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_directory = \"../datasets/\"\n",
    "filename = \"section_2.csv\"\n",
    "file_path = os.path.join(datasets_directory, filename)\n",
    "\n",
    "clean_df.to_csv(file_path, index=False, na_rep=\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "### Functions to Check the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_df(raw_dataf, clean_dataf):\n",
    "    raw_indexes = raw_dataf.index\n",
    "    clean_indexes = clean_dataf.index\n",
    "\n",
    "    dropped_indexes = raw_indexes[~raw_indexes.isin(clean_indexes)]\n",
    "    dropped_df = raw_dataf.loc[dropped_indexes].copy()\n",
    "\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropped DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = get_dropped_df(raw_df, clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Testing Code\n",
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Each Column for Parsing Errors and Standardize Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df[\"relationship_with_reference\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = clean_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(clean_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows\n",
    "Check the rows value counts to see if any rows are being dropped that should not be.\n",
    "Another way to check is by opening the variable inspector and manually scrolling\n",
    "through the `df_dropped` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tells us that there are no files that contain no information for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw File Count: {}\".format(len(raw_df[\"filename\"].value_counts())))\n",
    "print(\"Clean File Count: {}\".format(len(clean_df[\"filename\"].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see which rows were dropped based on null values in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df[dropped_df.isnull().sum(axis=1) < 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the value counts for each row to see if there are any anomalous (non-null values) that may have been incorrectly extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = dropped_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(dropped_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = clean_df.columns[clean_df.columns != \"filename\"]\n",
    "clean_df[clean_df.loc[:, df_columns].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

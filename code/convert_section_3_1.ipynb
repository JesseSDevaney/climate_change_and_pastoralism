{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = \"../surveys/plain_txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interviewee(contents):\n",
    "    name_regex = r\"nombre de la persona[^\\n:]*:[\\s]*([^\\n]*)\\n\"\n",
    "    name_search = re.findall(name_regex, contents)\n",
    "    if len(name_search) == 1:\n",
    "        name = name_search[0]\n",
    "        if name == \"3\":\n",
    "            name = \"N/A\"\n",
    "    else:\n",
    "        alt_name_regex = r\"nombre de la persona[^\\n\\(]*\\(\\w* +([^\\n]*)\\n\"\n",
    "        alt_name_search = re.findall(alt_name_regex, contents)\n",
    "        if len(alt_name_search) == 1:\n",
    "            name = alt_name_search[0]\n",
    "            if name == \"3\":\n",
    "                name = \"N/A\"\n",
    "        else:\n",
    "            name = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    name = name.strip()\n",
    "            \n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_section_3_1(contents):\n",
    "    section_3_1_pattern = r\"(bovinos machos[\\w\\W]*)iii-2\"\n",
    "    section_3_1_search = re.findall(section_3_1_pattern, contents)\n",
    "    \n",
    "    animal_list = [\"bovines_m\", \"bovines_f\",\n",
    "                   \"camelids_m\", \"camelids_f\",\n",
    "                   \"sheep_m\", \"sheep_f\",\n",
    "                   \"goats_m\", \"goats_f\",\n",
    "                   \"donkeys_m\", \"donkeys_f\",\n",
    "                   \"equines_m\", \"equines_f\",\n",
    "                   \"pigs_m\", \"pigs_f\"]\n",
    "    \n",
    "    entry = {}\n",
    "\n",
    "    if len(section_3_1_search) == 1:\n",
    "        section_3_1_str = section_3_1_search[0]\n",
    "        \n",
    "        fields_regex = r\"[\\|il_][_ \\t]*([\\d]*)[_ \\t]*[\\|il][^\\n\\d]*\\|?[il]?[_ \\t]*([\\d]*)[_ \\t]*\\|?[il]?[ \\t]*\\n\"\n",
    "        section_3_1 = re.findall(fields_regex, section_3_1_str)\n",
    "\n",
    "        if len(section_3_1) == 14:\n",
    "\n",
    "            row_count = 0\n",
    "            for row in section_3_1:\n",
    "                animal = animal_list[row_count]\n",
    "                entry[animal] = row\n",
    "                row_count += 1\n",
    "        else:\n",
    "            for animal in animal_list:\n",
    "                entry[animal] = \"UNENCOUNTERED FORMAT\"\n",
    "    else:\n",
    "        for animal in animal_list:\n",
    "            entry[animal] = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Whole File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(source_directory):\n",
    "    dataset = []\n",
    "\n",
    "    for process_file in os.listdir(source_directory):\n",
    "        file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "        # with statements automatically control the closing of files\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            contents = contents.lower()\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "            section_3_1_entries = parse_section_3_1(contents)\n",
    "            \n",
    "            if len(section_3_1_entries) != 14:\n",
    "                print(f\"Unencountered Format: {process_file}\")\n",
    "            \n",
    "            for key, value in section_3_1_entries.items():\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = parse_interviewee(contents)\n",
    "                data_dict[\"animal\"] = key\n",
    "                data_dict[\"current\"] = value[0]\n",
    "                data_dict[\"last_year\"] = value[1]\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "    # convert list to DataFrame\n",
    "    raw_df = pd.DataFrame(data=dataset)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def map_values(value):\n",
    "            if value <= 10:\n",
    "                return 1\n",
    "            elif value <= 50:\n",
    "                return 2\n",
    "            elif value <= 100:\n",
    "                return 3\n",
    "            elif value <= 150:\n",
    "                return 4\n",
    "            elif value <= 200:\n",
    "                return 5\n",
    "            elif value <= 300:\n",
    "                return 6\n",
    "            elif value <= 500:\n",
    "                return 7\n",
    "            elif value <= 700:\n",
    "                return 8\n",
    "            elif value <= 1000:\n",
    "                return 9\n",
    "            elif value > 1000:\n",
    "                return 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    return dataf.copy() \n",
    "\n",
    "\n",
    "def handle_null_data(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "    dataf = dataf.replace(\"0\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = isnull_sum > 1\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_columns(dataf):\n",
    "    # find the entries that contain numbers larger than 10 (the max class)\n",
    "    # use this filter as indicator of which interviewees used actual quantity\n",
    "    # versus those who specified quantity by the class system\n",
    "    # Then, map actual quantities to the class system\n",
    "    mapping_filter = (dataf[\"current\"] > 10) | (dataf[\"last_year\"] > 10)\n",
    "    files_to_map = dataf.loc[mapping_filter, \"filename\"].value_counts().index\n",
    "    file_filter = dataf[\"filename\"].isin(files_to_map)\n",
    "\n",
    "    current_mapped = dataf[\"current\"].apply(map_values)\n",
    "    last_year_mapped = dataf[\"last_year\"].apply(map_values)\n",
    "    \n",
    "    # create new column containing just the numbers which were numeric\n",
    "    dataf[\"current_numeric\"] = dataf[\"current\"].where(file_filter, np.NaN)\n",
    "    dataf[\"last_year_numeric\"] = dataf[\"last_year\"].where(file_filter, np.NaN)\n",
    "    \n",
    "    # map all values to their respective classes\n",
    "    dataf[\"current_class\"] = dataf[\"current\"].where(~file_filter, current_mapped)\n",
    "    dataf[\"last_year_class\"] = dataf[\"last_year\"].where(~file_filter, last_year_mapped)\n",
    "    \n",
    "    \n",
    "    # drop original columns\n",
    "    dataf = dataf.drop([\"current\", \"last_year\"], axis=1)\n",
    "    \n",
    "    return dataf\n",
    "\n",
    "\n",
    "def set_dtypes(dataf):\n",
    "    dataf[\"current\"] = dataf[\"current\"].astype(float)\n",
    "    dataf[\"last_year\"] = dataf[\"last_year\"].astype(float)\n",
    "    \n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw and Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = process_files(source_directory)\n",
    "\n",
    "clean_df = (raw_df\n",
    "            .pipe(start_pipeline)\n",
    "            .pipe(handle_null_data)\n",
    "            .pipe(set_dtypes)\n",
    "            .pipe(clean_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_directory = \"../datasets/\"\n",
    "filename = \"section_3_1.csv\"\n",
    "file_path = os.path.join(datasets_directory, filename)\n",
    "\n",
    "clean_df.to_csv(file_path, index=False, na_rep=\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "### Functions to Check the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_df(raw_dataf, clean_dataf):\n",
    "    raw_indexes = raw_dataf.index\n",
    "    clean_indexes = clean_dataf.index\n",
    "    \n",
    "    dropped_indexes = raw_indexes[~raw_indexes.isin(clean_indexes)]\n",
    "    dropped_df = raw_dataf.loc[dropped_indexes].copy()\n",
    "\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropped DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = get_dropped_df(raw_df, clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Testing Code\n",
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = clean_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(clean_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows\n",
    "Another way to check is by opening the variable inspector and manually scrolling\n",
    "through the `df_dropped` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tells us whether or not there are files that contain no information for this section. If raw file count is the same as clean file count, then no files were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw File Count: {}\".format(len(raw_df[\"filename\"].value_counts())))\n",
    "print(\"Clean File Count: {}\".format(len(clean_df[\"filename\"].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are files that are completely dropped, find those files and look at them to check for parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = raw_df[\"filename\"].value_counts().index\n",
    "clean_file_names = clean_df[\"filename\"].value_counts().index\n",
    "\n",
    "raw_file_names[~raw_file_names.isin(clean_file_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows value counts to see if any rows are being dropped that should not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = dropped_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(dropped_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = clean_df.columns[clean_df.columns != \"filename\"]\n",
    "clean_df[clean_df.loc[:, df_columns].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

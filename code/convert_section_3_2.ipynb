{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = \"../surveys/plain_txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between(str_1, str_2, contents):\n",
    "    regex_between = str_1 + r\"([\\w\\W]*)\" + str_2\n",
    "    regex_search = re.findall(regex_between, contents)\n",
    "    \n",
    "    return regex_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interviewee(contents):\n",
    "    name_regex = r\"nombre de la persona[^\\n:]*:[\\s]*([^\\n]*)\\n\"\n",
    "    name_search = re.findall(name_regex, contents)\n",
    "    if len(name_search) == 1:\n",
    "        name = name_search[0]\n",
    "        if name == \"3\":\n",
    "            name = \"N/A\"\n",
    "    else:\n",
    "        alt_name_regex = r\"nombre de la persona[^\\n\\(]*\\(\\w* +([^\\n]*)\\n\"\n",
    "        alt_name_search = re.findall(alt_name_regex, contents)\n",
    "        if len(alt_name_search) == 1:\n",
    "            name = alt_name_search[0]\n",
    "            if name == \"3\":\n",
    "                name = \"N/A\"\n",
    "        else:\n",
    "            name = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    name = name.strip()\n",
    "            \n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_date_range(contents):\n",
    "    date_range = \"\"\n",
    "    \n",
    "    section_start = \"iii-2\"\n",
    "    section_end = \"iii-3\"\n",
    "    section_search = find_between(section_start, section_end, contents)\n",
    "    \n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0]\n",
    "        \n",
    "        date_regex = r\"([\\w ]+\\d+ *[-–] *[\\w ]+\\d+)\"\n",
    "        date_search = re.findall(date_regex, section_string)\n",
    "        \n",
    "        if len(date_search) == 1:\n",
    "            date_range = date_search[0]\n",
    "        else:\n",
    "            date_range = \"UNENCOUNTERED FORMAT\"\n",
    "    else:\n",
    "        date_range = \"UNENCOUNTERED FORMAT\"\n",
    "    \n",
    "    return date_range\n",
    "\n",
    "\n",
    "def parse_section_3_2_outflow(contents):\n",
    "    regex_double_string = r\"salidas de stocks\\n\\nmortalidad animal([\\w\\W]*)entradas de stocks[\\w\\W]*iii-3\"\n",
    "    regex_double_list = re.findall(regex_double_string, contents)\n",
    "    \n",
    "    regex_single_string = r\"salidas de stocks\\nmortalidad animal([\\w\\W]*)entradas de stocks[\\w\\W]*iii-3\"\n",
    "    regex_single_list = re.findall(regex_single_string, contents)\n",
    "\n",
    "    entry_fields = [\"bovines\", \"sheep\", \"goats\", \"camelids\", \n",
    "                    \"donkeys\", \"equines\", \"pigs\"]\n",
    "\n",
    "    outflow_entry_list = []\n",
    "    outflow_entry = {}\n",
    "    \n",
    "    outflow_row_labels = [\"animal_death\", \"robbery\", \n",
    "                          \"slaughter_or_consumption\", \"loans_given\", \n",
    "                          \"sold\", \"others_outflow\"]\n",
    "\n",
    "    outflow_data = []\n",
    "    row = {}\n",
    "        \n",
    "    \n",
    "    if len(regex_double_list) == 1:\n",
    "        outflow = regex_double_list[0]\n",
    "        \n",
    "        if re.search(\"\\n\", outflow):\n",
    "            outflow_list = outflow.split(\"\\n\")\n",
    "            \n",
    "            count = -2\n",
    "            \n",
    "            # handle a file structure where some files have two extra newlines \n",
    "            # at the end of each row\n",
    "            count_restart = -6\n",
    "            if outflow_list.index(\"robos\") < 20:\n",
    "                count_restart = -4\n",
    "            \n",
    "            for item in outflow_list:\n",
    "                item = item.strip()\n",
    "                if count == 14:\n",
    "                    outflow_entry_list.append(outflow_entry)\n",
    "                    outflow_entry = {}\n",
    "                    count = count_restart\n",
    "                if count % 2 == 0 & count >= 0:\n",
    "                    entry_field = entry_fields[count//2]\n",
    "                    outflow_entry[entry_field] = item\n",
    "                count += 1\n",
    "            \n",
    "            if len(outflow_entry_list) == 6:\n",
    "                for i in range(len(outflow_row_labels)):\n",
    "                    label = outflow_row_labels[i]\n",
    "                    entry = outflow_entry_list[i]\n",
    "                    for key, value in entry.items():\n",
    "                        row = {}\n",
    "                        row[\"stock_variation\"] = label\n",
    "                        row[\"animal\"] = key\n",
    "                        row[\"value\"] = value\n",
    "                        outflow_data.append(row)\n",
    "            else:\n",
    "                outflow_data = [{}]\n",
    "        else:\n",
    "            outflow_data = [{}]\n",
    "    elif len(regex_single_list) == 1:\n",
    "        outflow = regex_single_list[0]\n",
    "        \n",
    "        if re.search(\"\\n\", outflow):\n",
    "            outflow_list = outflow.split(\"\\n\")\n",
    "\n",
    "            # count until 7\n",
    "            count = -1\n",
    "            for item in outflow_list:\n",
    "                item = item.strip()\n",
    "                if count == 7:\n",
    "                    outflow_entry_list.append(outflow_entry)\n",
    "                    outflow_entry = {}\n",
    "                    count = -3\n",
    "                if count >= 0:\n",
    "                    entry_field = entry_fields[count]\n",
    "                    outflow_entry[entry_field] = item\n",
    "                count += 1\n",
    "            \n",
    "            if len(outflow_entry_list) == 6:\n",
    "                for i in range(len(outflow_row_labels)):\n",
    "                    label = outflow_row_labels[i]\n",
    "                    entry = outflow_entry_list[i]\n",
    "                    for key, value in entry.items():\n",
    "                        row = {}\n",
    "                        row[\"stock_variation\"] = label\n",
    "                        row[\"animal\"] = key\n",
    "                        row[\"value\"] = value\n",
    "                        outflow_data.append(row)\n",
    "            else:\n",
    "                outflow_data = [{}]\n",
    "        else:\n",
    "            outflow_data = [{}]\n",
    "    else:\n",
    "        outflow_data = [{}]\n",
    "\n",
    "    return outflow_data\n",
    "\n",
    "\n",
    "def parse_section_3_2_inflow(contents):\n",
    "    regex_double_string = r\"entradas de stocks\\n\\npartos \\(nacimientos\\)([\\w\\W]*)iii-3\"\n",
    "    regex_double_list = re.findall(regex_double_string, contents)\n",
    "\n",
    "    regex_single_string = r\"entradas de stocks\\npartos \\(nacimientos\\)([\\w\\W]*)iii-3\"\n",
    "    regex_single_list = re.findall(regex_single_string, contents)\n",
    "    \n",
    "    entry_fields = [\"bovines\", \"sheep\", \"goats\", \"camelids\", \n",
    "                    \"donkeys\", \"equines\", \"pigs\"]\n",
    "\n",
    "    inflow_entry_list = []\n",
    "    inflow_entry = {}\n",
    "    \n",
    "    inflow_row_labels = [\"births\", \"loans_received\", \n",
    "                         \"purchased\", \"others_inflow\"]\n",
    "\n",
    "    inflow_data = []\n",
    "    row = {}\n",
    "    \n",
    "    if len(regex_double_list) == 1:\n",
    "        inflow = regex_double_list[0]\n",
    "\n",
    "        if re.search(\"\\n\", inflow):\n",
    "            inflow_list = inflow.split(\"\\n\")\n",
    "\n",
    "            count = -2\n",
    "            # handle a file structure where some files have two extra newlines \n",
    "            # at the end of each row\n",
    "            count_restart = -6\n",
    "            if inflow_list.index(\"préstamos recibidos\") < 20:\n",
    "                count_restart = -4\n",
    "            \n",
    "            # count until 14, skip odd indexes (1, 3, 5, 7, ...)\n",
    "            for item in inflow_list:\n",
    "                item = item.strip()\n",
    "                if count == 14:\n",
    "                    inflow_entry_list.append(inflow_entry)\n",
    "                    inflow_entry = {}\n",
    "                    count = count_restart\n",
    "                if count % 2 == 0 & count >= 0:\n",
    "                    entry_field = entry_fields[count//2]\n",
    "                    inflow_entry[entry_field] = item\n",
    "                count += 1\n",
    "\n",
    "            if len(inflow_entry_list) == 4:\n",
    "                for i in range(len(inflow_row_labels)):\n",
    "                    label = inflow_row_labels[i]\n",
    "                    entry = inflow_entry_list[i]\n",
    "                    for key, value in entry.items():\n",
    "                        row = {}\n",
    "                        row[\"stock_variation\"] = label\n",
    "                        row[\"animal\"] = key\n",
    "                        row[\"value\"] = value\n",
    "                        inflow_data.append(row)\n",
    "            else:\n",
    "                inflow_data = [{}]\n",
    "        else:\n",
    "            inflow_data = [{}]\n",
    "    elif len(regex_single_list) == 1:\n",
    "        inflow = regex_single_list[0]\n",
    "        \n",
    "        if re.search(\"\\n\", inflow):\n",
    "            inflow_list = inflow.split(\"\\n\")\n",
    "\n",
    "            # count until 7\n",
    "            count = -1\n",
    "            for item in inflow_list:\n",
    "                item = item.strip()\n",
    "                if count == 7:\n",
    "                    inflow_entry_list.append(inflow_entry)\n",
    "                    inflow_entry = {}\n",
    "                    count = -3\n",
    "                if count >= 0:\n",
    "                    entry_field = entry_fields[count]\n",
    "                    inflow_entry[entry_field] = item\n",
    "                count += 1\n",
    "            \n",
    "            if len(inflowentry_list) == 6:\n",
    "                for i in range(len(inflow_row_labels)):\n",
    "                    label = inflow_row_labels[i]\n",
    "                    entry = inflow_entry_list[i]\n",
    "                    for key, value in entry.items():\n",
    "                        row = {}\n",
    "                        row[\"stock_variation\"] = label\n",
    "                        row[\"animal\"] = key\n",
    "                        row[\"value\"] = value\n",
    "                        inflow_data.append(row)\n",
    "            else:\n",
    "                inflow_data = [{}]\n",
    "        else:\n",
    "            inflow_data = [{}]\n",
    "    else:\n",
    "        inflow_data = [{}]\n",
    "    \n",
    "    return inflow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Whole File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(source_directory):\n",
    "    dataset = []\n",
    "\n",
    "    for process_file in os.listdir(source_directory):\n",
    "        file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "        # with statements automatically control the closing of files\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            contents = contents.lower()\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "            interviewee = parse_interviewee(contents)\n",
    "            date_range = parse_date_range(contents)\n",
    "            \n",
    "            outflow_data = parse_section_3_2_outflow(contents)\n",
    "            inflow_data = parse_section_3_2_inflow(contents)\n",
    "            \n",
    "            if len(outflow_data) != 42:\n",
    "                print(f\"Unencountered Format: {process_file}, Length of outflow_data: {len(outflow_data)}\")\n",
    "                \n",
    "            if len(inflow_data) != 28:\n",
    "                print(f\"Unencountered Format: {process_file}, Length of outflow_data: {len(inflow_data)}\")\n",
    "\n",
    "                \n",
    "            for row in outflow_data:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = interviewee\n",
    "                data_dict[\"date_range\"] = date_range\n",
    "                \n",
    "                for key, value in row.items():\n",
    "                    data_dict[key] = value\n",
    "                    \n",
    "                dataset.append(data_dict)\n",
    "            \n",
    "            \n",
    "            for row in inflow_data:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = interviewee\n",
    "                data_dict[\"date_range\"] = date_range\n",
    "                \n",
    "                for key, value in row.items():\n",
    "                    data_dict[key] = value\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "                \n",
    "    # convert list to DataFrame\n",
    "    raw_df = pd.DataFrame(data=dataset)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_range(dataf):\n",
    "    replace_dict = {\"agosto 2018 - agosto 2019\": \"august 2018-2019\",\n",
    "                    \"septiembre 2018 - septiembre 2019\": \"september 2018-2019\",\n",
    "                    \"octubre 2018 – octubre 2019\": \"october 2018-2019\",\n",
    "                    \"octubre 2018 - octubre 2019\": \"october 2018-2019\"}\n",
    "    dataf[\"date_range\"] = dataf[\"date_range\"].replace(replace_dict)\n",
    "    \n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    return dataf.copy() \n",
    "\n",
    "\n",
    "def handle_null_data(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "    \n",
    "    # drop rows which contain no information for value or other_value\n",
    "    drop_filter = (((dataf[\"value\"].isnull()) | (dataf[\"value\"] == \"0\"))\n",
    "                   & dataf[\"other_value\"].isnull())\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "    \n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_columns(dataf):\n",
    "    # create new column to keep the \"x\" values indicating that\n",
    "    # spot occured in that row, but we do not know how many\n",
    "    other_filter = (dataf[\"value\"] == \"x\")\n",
    "    dataf[\"other_value\"] = dataf[\"value\"].where(other_filter, np.NaN)\n",
    "    \n",
    "    # replace values with their corresponding numeric value.\n",
    "    # drop \"x\" values, because they are not numeric\n",
    "    map_dict = {\"1 (parto)\": \"1\",\n",
    "                \"2 (parto)\": \"2\",\n",
    "                \"x\": np.NaN,\n",
    "                \"o\": \"0\"}\n",
    "    dataf[\"value\"] = dataf[\"value\"].replace(map_dict)\n",
    "    \n",
    "    return dataf\n",
    "\n",
    "\n",
    "def set_dtypes(dataf):\n",
    "    dataf[\"value\"] = dataf[\"value\"].astype(float)\n",
    "    \n",
    "    return dataf\n",
    "\n",
    "\n",
    "def transform_values(dataf):\n",
    "    # outflow #\n",
    "    outflow_row_labels = [\"animal_death\", \"robbery\",\"slaughter_or_consumption\", \n",
    "                          \"loans_given\", \"sold\", \"others_outflow\"]\n",
    "    \n",
    "    # if flow_label in outflow_row_labels, change positive value to negative\n",
    "    outflow_filter = dataf[\"stock_variation\"].isin(outflow_row_labels)\n",
    "    flipped_values = -1 * dataf[\"value\"]\n",
    "    \n",
    "    dataf[\"value\"] = dataf[\"value\"].where(~outflow_filter, flipped_values)\n",
    "    \n",
    "    \n",
    "    dataf = transform_date_range(dataf)\n",
    "    \n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw and Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = process_files(source_directory)\n",
    "\n",
    "clean_df = (raw_df\n",
    "            .pipe(start_pipeline)\n",
    "            .pipe(clean_columns)\n",
    "            .pipe(handle_null_data)\n",
    "            .pipe(set_dtypes)\n",
    "            .pipe(transform_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_directory = \"../datasets/\"\n",
    "filename = \"section_3_2.csv\"\n",
    "file_path = os.path.join(datasets_directory, filename)\n",
    "\n",
    "clean_df.to_csv(file_path, index=False, na_rep=\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "### Functions to Check the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_df(raw_dataf, clean_dataf):\n",
    "    raw_indexes = raw_dataf.index\n",
    "    clean_indexes = clean_dataf.index\n",
    "    \n",
    "    dropped_indexes = raw_indexes[~raw_indexes.isin(clean_indexes)]\n",
    "    dropped_df = raw_dataf.loc[dropped_indexes].copy()\n",
    "\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropped DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = get_dropped_df(raw_df, clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Testing Code\n",
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = clean_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(clean_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows\n",
    "Another way to check is by opening the variable inspector and manually scrolling\n",
    "through the `df_dropped` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tells us whether or not there are files that contain no information for this section. If raw file count is the same as clean file count, then no files were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw File Count: {}\".format(len(raw_df[\"filename\"].value_counts())))\n",
    "print(\"Clean File Count: {}\".format(len(clean_df[\"filename\"].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are files that are completely dropped, find those files and look at them to check for parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = raw_df[\"filename\"].value_counts().index\n",
    "clean_file_names = clean_df[\"filename\"].value_counts().index\n",
    "\n",
    "raw_file_names[~raw_file_names.isin(clean_file_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows value counts to see if any rows are being dropped that should not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = dropped_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(dropped_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = clean_df.columns[clean_df.columns != \"filename\"]\n",
    "clean_df[clean_df.loc[:, df_columns].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = \"../surveys/plain_txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_list = [\"sequía \\/ pausas pluviométricas\",\n",
    "              \"lluvias intempestivas\", \"inundaciones\",\n",
    "              \"incendios de maleza\", \"estrés térmico\",\n",
    "              \"otros eventos de clima extremo\",\n",
    "              \"pérdidas de ganado por enfermedades animales\",\n",
    "              \"problema de acceso a servicios veterinarios\",\n",
    "              \"otros eventos extremos de salud\",\n",
    "              \"muerte de un miembro activo de la familia\",\n",
    "              \"enfermedades incapacitantes o accidente de un miembro activo del hogar\",\n",
    "              \"aumento del gasto sanitario\",\n",
    "              \"otros eventos extremos de salud\",\n",
    "              \"aumento de los precios de los piensos\",\n",
    "              \"aumento de los precios de los alimentos\",\n",
    "              \"disminución de los precios de venta de ganados \\/ productos agrícolas\",\n",
    "              \"interrupción de transferencias regulares de otros miembros del hogar\",\n",
    "              \"pérdidas significativas de ingresos complementario\",\n",
    "              \"pérdida de empleo por parte de un miembro de la familia\",\n",
    "              \"pérdidas de cultivos \\(invasión de langosta\\)\",\n",
    "              \"otros eventos económicos extremos\",\n",
    "              \"robo de ganado\",\n",
    "              \"saqueo de cultivos\",\n",
    "              \"conflicto \\/ violencia \\/ i?n?seguridad \\/ expropiación  \\(a nivel comunitario\\)\\*\",\n",
    "              \"otros eventos extremos \\(especificar\\)\",\n",
    "              \"\"]\n",
    "# sections: #1: 0-5, #2: 6-8, #3: 9-12, #4: 13-20, #5: 21-23, #6: 24\n",
    "shock_sections = [\"choques climáticos\", \"choque sanitario en el ganado\",\n",
    "                  \"choque sanitario en la familia\", \"choques économico\",\n",
    "                  \"choques por la seguridades\", \"other_extreme_events\"]\n",
    "# excluding nature_of_change column\n",
    "column_headers = [\"past_15_years_adversely_affected_by_change\",\n",
    "                  \"most_important_change\", \"income_affected\",\n",
    "                  \"heritage_affected\", \"food_production_affected\",\n",
    "                  \"food_reserves_affected\", \"food_purchases_affected\",\n",
    "                  \"first_most_important_coping_strategy\",\n",
    "                  \"second_most_important_coping_strategy\",\n",
    "                  \"third_most_important_coping_strategy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between(str_1, str_2, contents):\n",
    "    regex_between = str_1 + r\"([\\w\\W]*)\" + str_2\n",
    "    regex_search = re.findall(regex_between, contents)\n",
    "\n",
    "    return regex_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interviewee(contents):\n",
    "    name_regex = r\"nombre de la persona[^\\n:]*:[\\s]*([^\\n]*)\\n\"\n",
    "    name_search = re.findall(name_regex, contents)\n",
    "    if len(name_search) == 1:\n",
    "        name = name_search[0]\n",
    "        if name == \"3\":\n",
    "            name = \"N/A\"\n",
    "    else:\n",
    "        alt_name_regex = r\"nombre de la persona[^\\n\\(]*\\(\\w* +([^\\n]*)\\n\"\n",
    "        alt_name_search = re.findall(alt_name_regex, contents)\n",
    "        if len(alt_name_search) == 1:\n",
    "            name = alt_name_search[0]\n",
    "            if name == \"3\":\n",
    "                name = \"N/A\"\n",
    "        else:\n",
    "            name = \"UNENCOUNTERED FORMAT\"\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_section_7(section_search, shock_list,\n",
    "                    shock_sections, column_headers):\n",
    "    section_7_data = []\n",
    "\n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0] + \"\"\n",
    "        # used to parse only the number for each column\n",
    "        entry_regex = r\"([|\\d\\w -]+)\"\n",
    "        entry_empty_regex = r\"(?<!\\d)\\|[ _]*\\|(?!\\d)\"\n",
    "\n",
    "        for i in range(len(shock_list)-1):\n",
    "            item_dict = {}\n",
    "            item_1 = shock_list[i]\n",
    "            item_2 = shock_list[i+1]\n",
    "\n",
    "            row_search = find_between(item_1, item_2, section_string)\n",
    "\n",
    "            # sections: #1: 0-5, #2: 6-8, #3: 9-12, #4: 13-20, #5: 21-23,\n",
    "            #           #6: 24\n",
    "            shock_section = \"\"\n",
    "            if i <= 5:\n",
    "                shock_section = shock_sections[0]\n",
    "            elif i >= 6 and i <= 8:\n",
    "                shock_section = shock_sections[1]\n",
    "            elif i >= 9 and i <= 12:\n",
    "                shock_section = shock_sections[2]\n",
    "            elif i >= 13 and i <= 20:\n",
    "                shock_section = shock_sections[3]\n",
    "            elif i >= 21 and i <= 23:\n",
    "                shock_section = shock_sections[4]\n",
    "            else:\n",
    "                shock_section = shock_sections[5]\n",
    "            item_dict[\"shock_section\"] = shock_section\n",
    "            item_dict[\"shock\"] = item_1\n",
    "\n",
    "            # check if survey contains this row, because not all surveys\n",
    "            # have this row as an option\n",
    "            if (item_2 == \"aumento de los precios de los piensos\"\n",
    "               and len(row_search) == 0):\n",
    "                item_2 = shock_list[i+2]\n",
    "                row_search = find_between(item_1, item_2, section_string)\n",
    "            elif (item_1 == \"aumento de los precios de los piensos\"\n",
    "                  and len(row_search) == 0):\n",
    "                for col in column_headers:\n",
    "                    item_dict[col] = \"ROW NOT AVAILABLE\"\n",
    "                section_7_data.append(item_dict)\n",
    "                continue\n",
    "\n",
    "            # if expense row found\n",
    "            if len(row_search) == 1:\n",
    "                row_string = row_search[0]\n",
    "                row_list = row_string.split(\"\\n\")\n",
    "                row_list_len = len(row_list)\n",
    "\n",
    "                if row_list_len >= 24:\n",
    "                    count = -2\n",
    "\n",
    "                    for j in range(row_list_len):\n",
    "                        if count > 18:\n",
    "                            break\n",
    "                        if count >= 0 and count % 2 == 0:\n",
    "                            col = column_headers[count//2]\n",
    "                            entry = row_list[j]\n",
    "\n",
    "                            entry_search = re.findall(entry_regex, entry)\n",
    "                            entry_empty_search = re.findall(entry_empty_regex,\n",
    "                                                            entry)\n",
    "                            if len(entry_empty_search) == 1:\n",
    "                                item_dict[col] = \"NO VALUE ENTERED\"\n",
    "                            elif len(entry_search) == 1 and entry != \" \":\n",
    "                                item_dict[col] = entry_search[0]\n",
    "                            elif entry == \"\":\n",
    "                                item_dict[col] = \"EMPTY STRING\"\n",
    "                            else:\n",
    "                                item_dict[col] = \"UNENCOUNTERED FORMAT (ENTRY NOT FOUND)\"\n",
    "                        count += 1\n",
    "\n",
    "                    # parse otros additional info\n",
    "                    if item_1 == \"otros eventos extremos \\(especificar\\)\":\n",
    "                        otros_regex = r\"otros eventos extremos \\(especificar\\):? ?([^\\n]*)\"\n",
    "                        otros_search = re.findall(otros_regex, section_string)\n",
    "\n",
    "                        if len(otros_search) == 1:\n",
    "                            otros_string = otros_search[0]\n",
    "                            item_dict[\"other_info\"] = otros_string\n",
    "                    elif item_1 == \"otros eventos de clima extremo\":\n",
    "                        otros_regex = r\"otros eventos de clima extremo:? ?([^\\n]*)\"\n",
    "                        otros_search = re.findall(otros_regex, section_string)\n",
    "\n",
    "                        if len(otros_search) == 1:\n",
    "                            otros_string = otros_search[0]\n",
    "                            item_dict[\"other_info\"] = otros_string\n",
    "                else:\n",
    "                    for col in column_headers:\n",
    "                        item_dict[col] = \"UNENCOUNTERED FORMAT (ROW NOT LONG ENOUGH)\"\n",
    "            else:\n",
    "                for col in column_headers:\n",
    "                    item_dict[col] = \"UNENCOUNTERED FORMAT (ROW NOT FOUND)\"\n",
    "\n",
    "            section_7_data.append(item_dict)\n",
    "    else:\n",
    "        section_7_data = [{}]\n",
    "\n",
    "    return section_7_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Whole File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(source_directory, shock_list,\n",
    "                  shock_sections, column_headers):\n",
    "    dataset = []\n",
    "\n",
    "    for process_file in os.listdir(source_directory):\n",
    "        file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "        # with statements automatically control the closing of files\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            contents = contents.lower()\n",
    "\n",
    "            section_start = \"sección vii:\"\n",
    "            # empty because it is the last section\n",
    "            section_end = \"\"\n",
    "            # extract only the relevant section\n",
    "            section_search = find_between(section_start, section_end, contents)\n",
    "\n",
    "            interviewee = parse_interviewee(contents)\n",
    "            section_7_data = parse_section_7(section_search, shock_list,\n",
    "                                             shock_sections, column_headers)\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "            # check to make sure files contain the right amount of entries\n",
    "            if len(section_7_data) <= 1:\n",
    "                print(f\"Unencountered Format: {process_file}\\n\"\n",
    "                      + f\"Length of section_data: {len(section_7_data)}\\n\")\n",
    "\n",
    "            for row in section_7_data:\n",
    "                data_dict = {}\n",
    "                data_dict[\"filename\"] = process_file\n",
    "                data_dict[\"interviewee\"] = interviewee\n",
    "\n",
    "                for key, value in row.items():\n",
    "                    data_dict[key] = value\n",
    "\n",
    "                dataset.append(data_dict)\n",
    "\n",
    "    # convert list to DataFrame\n",
    "    raw_df = pd.DataFrame(data=dataset)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_shock_section(dataf):\n",
    "    replace_dict = {\"choques climáticos\": \"climate_shocks\",\n",
    "                    \"choque sanitario en el ganado\": \"sanitary_shock_in_livestock\",\n",
    "                    \"choque sanitario en la familia\": \"health_shock_in_family\",\n",
    "                    \"choques économico\": \"economic_shock\",\n",
    "                    \"choques por la seguridades\": \"security_shock\"}\n",
    "    dataf[\"shock_section\"] = dataf[\"shock_section\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_shock(dataf):\n",
    "    replace_dict = {\"sequía \\/ pausas pluviométricas\": \"droughts\",\n",
    "                    \"lluvias intempestivas\": \"untimely_rain\",\n",
    "                    \"inundaciones\": \"floods\",\n",
    "                    \"incendios de maleza\": \"brush_fires\",\n",
    "                    \"estrés térmico\": \"heat_stress\",\n",
    "                    \"otros eventos de clima extremo\": \"other_extreme_weather_events\",\n",
    "                    \"pérdidas de ganado por enfermedades animales\": \"livestock_loss_from_animal_disease\",\n",
    "                    \"problema de acceso a servicios veterinarios\": \"access_to_veterinary_service_problem\",\n",
    "                    \"otros eventos extremos de salud\": \"other_extreme_health_events\",\n",
    "                    \"muerte de un miembro activo de la familia\": \"death_of_an_active_family_member\",\n",
    "                    \"enfermedades incapacitantes o accidente de un miembro activo del hogar\": \"disabling_illness_or_accident_to_an_active_member_of_the_household\",\n",
    "                    \"aumento del gasto sanitario\": \"increase_in_healthcare_spending\",\n",
    "                    \"aumento de los precios de los piensos\": \"increase_in_feed_prices\",\n",
    "                    \"aumento de los precios de los alimentos\": \"increase_in_food_prices\",\n",
    "                    \"disminución de los precios de venta de ganados \\/ productos agrícolas\": \"decrease_in_sale_price_of_livestock_and_agricultural_products\",\n",
    "                    \"interrupción de transferencias regulares de otros miembros del hogar\": \"interruption_of_regular_transfers_of_other_household_members\",\n",
    "                    \"pérdidas significativas de ingresos complementario\": \"significant_loss_of_supplemental_income\",\n",
    "                    \"pérdida de empleo por parte de un miembro de la familia\": \"loss_of_employment_by_a_family_member\",\n",
    "                    \"pérdidas de cultivos \\(invasión de langosta\\)\": \"crop_losses_locust_invasion\",\n",
    "                    \"otros eventos económicos extremos\": \"other_extreme_economic_events\",\n",
    "                    \"robo de ganado\": \"livestock_theft\",\n",
    "                    \"saqueo de cultivos\": \"crop_plunder\",\n",
    "                    \"conflicto \\/ violencia \\/ i?n?seguridad \\/ expropiación  \\(a nivel comunitario\\)\\*\": \"conflict_violence_insecurity_expropriation_at_a_community_level\",\n",
    "                    \"otros eventos extremos \\(especificar\\)\": \"other_extreme_events\"}\n",
    "    dataf[\"shock\"] = dataf[\"shock\"].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def remove_column_formatting(dataf, col):\n",
    "    dataf[col] = dataf[col].str.replace(\"-\", \"\")\n",
    "    dataf[col] = dataf[col].str.replace(\"_\", \"\")\n",
    "    dataf[col] = dataf[col].str.replace(\"|\", \"\")\n",
    "    dataf[col] = dataf[col].str.strip()\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_past_15_years(dataf):\n",
    "    col = \"past_15_years_adversely_affected_by_change\"\n",
    "\n",
    "    # remove formatting\n",
    "    dataf = remove_column_formatting(dataf, col)\n",
    "\n",
    "    replace_dict = {\"1\": \"yes\",\n",
    "                    \"2\": \"no\",\n",
    "                    \"si\": \"yes\",\n",
    "                    \"i\": \"yes\",\n",
    "                    \"11\": \"yes\"}\n",
    "\n",
    "    dataf[col] = dataf[col].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_most_important_change(dataf):\n",
    "    col = \"most_important_change\"\n",
    "\n",
    "    # remove formatting\n",
    "    dataf = remove_column_formatting(dataf, col)\n",
    "\n",
    "    replace_dict = {\"2 y 3\": \"2,3\",\n",
    "                    \"1 y 3\": \"1,3\"}\n",
    "\n",
    "    dataf[col] = dataf[col].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_column_3(dataf):\n",
    "    cols = [\"income_affected\", \"heritage_affected\", \"food_production_affected\",\n",
    "            \"food_reserves_affected\", \"food_purchases_affected\"]\n",
    "\n",
    "    # remove formatting\n",
    "    for col in cols:\n",
    "        dataf = remove_column_formatting(dataf, col)\n",
    "\n",
    "    replace_dict = {\"1\": \"increase\",\n",
    "                    \"2\": \"decrease\",\n",
    "                    \"3\": \"no_change\"}\n",
    "\n",
    "    for col in cols:\n",
    "        dataf[col] = dataf[col].replace(replace_dict)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_column_4(dataf):\n",
    "    cols = [\"first_most_important_coping_strategy\",\n",
    "            \"second_most_important_coping_strategy\",\n",
    "            \"third_most_important_coping_strategy\"]\n",
    "\n",
    "    # remove formatting\n",
    "    for col in cols:\n",
    "        dataf = remove_column_formatting(dataf, col)\n",
    "\n",
    "    replace_dict = {\"1\": \"greater_mobility\",\n",
    "                    \"2\": \"greater_use_of_family_work\",\n",
    "                    \"3\": \"use_of_salaried_work\",\n",
    "                    \"4\": \"greater_adoption_of_more_resistant_animal_speicies\",\n",
    "                    \"5\": \"establishment_of_private_enclosures_for_forage_crops\",\n",
    "                    \"6\": \"establishment_of_private_enclosures_for_market_gardening\",\n",
    "                    \"7\": \"use_insurance_and_request_expert_support\",\n",
    "                    \"8\": \"eat_alternative_foods\",\n",
    "                    \"9\": \"reduce_number_of_meals_per_day\",\n",
    "                    \"01\": \"greater_mobility\",\n",
    "                    \"02\": \"greater_use_of_family_work\",\n",
    "                    \"03\": \"use_of_salaried_work\",\n",
    "                    \"04\": \"greater_adoption_of_more_resistant_animal_speicies\",\n",
    "                    \"05\": \"establishment_of_private_enclosures_for_forage_crops\",\n",
    "                    \"06\": \"establishment_of_private_enclosures_for_market_gardening\",\n",
    "                    \"07\": \"use_insurance_and_request_expert_support\",\n",
    "                    \"08\": \"eat_alternative_foods\",\n",
    "                    \"09\": \"reduce_number_of_meals_per_day\",\n",
    "                    \"10\": \"reduce_food_eaten_by_adults_to_benefit_children\",\n",
    "                    \"11\": \"sale_of_animals_depopulation\",\n",
    "                    \"12\": \"sale_of_production_goods\",\n",
    "                    \"13\": \"sale_of_non_productive_assets\",\n",
    "                    \"14\": \"sale_of_food_reserves\",\n",
    "                    \"15\": \"decrease_in_the_sale_of_animals\",\n",
    "                    \"16\": \"other_AGRs\",\n",
    "                    \"17\": \"complementary_job_search\",\n",
    "                    \"18\": \"help_from_family_members\",\n",
    "                    \"19\": \"help_from_community_and_associations\",\n",
    "                    \"20\": \"loans\",\n",
    "                    \"21\": \"state_received_aid\",\n",
    "                    \"22\": \"ngo_and_project_support\",\n",
    "                    \"23\": \"unusual_short_migration_<_6_months\",\n",
    "                    \"24\": \"unusual_long_migration_>_6_months\",\n",
    "                    \"25\": \"definitive_migration\",\n",
    "                    \"26\": \"children_trust_in_foster_families\",\n",
    "                    \"27\": \"other_strategy\",\n",
    "                    \"28\": \"no_strategy\",}\n",
    "\n",
    "    for col in cols:\n",
    "        dataf[col] = dataf[col].replace(replace_dict)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    return dataf.copy()\n",
    "\n",
    "\n",
    "def handle_null_data(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"NO VALUE ENTERED\", np.NaN)\n",
    "    dataf = dataf.replace(\"EMPTY STRING\", np.NaN)\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = ((isnull_sum >= 11)\n",
    "                   | ((isnull_sum >= 9)\n",
    "                      & ((dataf[\"past_15_years_adversely_affected_by_change\"] == \"no\")\n",
    "                         | (dataf[\"past_15_years_adversely_affected_by_change\"].isnull())))\n",
    "                   | ((isnull_sum >= 9)\n",
    "                      & ((dataf[\"past_15_years_adversely_affected_by_change\"] == \"yes\"))))\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def clean_values(dataf):\n",
    "    dataf = clean_shock_section(dataf)\n",
    "    dataf = clean_shock(dataf)\n",
    "    dataf = clean_past_15_years(dataf)\n",
    "    dataf = clean_most_important_change(dataf)\n",
    "    dataf = clean_column_3(dataf)\n",
    "    dataf = clean_column_4(dataf)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw and Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = process_files(source_directory, shock_list,\n",
    "                       shock_sections, column_headers)\n",
    "\n",
    "clean_df = (raw_df\n",
    "            .pipe(start_pipeline)\n",
    "            .pipe(clean_values)\n",
    "            .pipe(handle_null_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "### Functions to Check the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_df(raw_dataf, clean_dataf):\n",
    "    raw_indexes = raw_dataf.index\n",
    "    clean_indexes = clean_dataf.index\n",
    "\n",
    "    dropped_indexes = raw_indexes[~raw_indexes.isin(clean_indexes)]\n",
    "    dropped_df = raw_dataf.loc[dropped_indexes].copy()\n",
    "\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropped DataFrame\n",
    "Create the `dropped_df` and collect the rows which contain no non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = get_dropped_df(raw_df, clean_df)\n",
    "\n",
    "# get rows that were dropped but do not contain non-null values\n",
    "# (only \"EMPTY STRING\")\n",
    "check_filter = (dropped_df == \"EMPTY STRING\").sum(axis=1) == 10\n",
    "null_df = dropped_df.loc[check_filter, [\"filename\", \"shock\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropped_row(section_search, shock, shock_list):\n",
    "    dropped_row = {}\n",
    "\n",
    "    if len(section_search) == 1:\n",
    "        section_string = section_search[0] + \"\"\n",
    "\n",
    "        dropped_row[\"shock\"] = shock\n",
    "\n",
    "        shock_index = shock_list.index(shock)\n",
    "        item_1 = shock_list[shock_index]\n",
    "        item_2 = shock_list[shock_index + 1]\n",
    "\n",
    "        row_search = find_between(item_1, item_2, section_string)\n",
    "\n",
    "        # if expense row found\n",
    "        if len(row_search) == 1:\n",
    "            row_string = row_search[0]\n",
    "            row_list = row_string.split(\"\\n\")\n",
    "            row_list_len = len(row_list)\n",
    "\n",
    "            if row_list_len >= 24:\n",
    "                count = -2\n",
    "                for j in range(row_list_len):\n",
    "                    if count > 18:\n",
    "                        break\n",
    "                    if count >= 0:\n",
    "                        col = str(count)\n",
    "                        entry = row_list[j]\n",
    "                        entry = entry.strip()\n",
    "                        dropped_row[col] = entry\n",
    "                    count += 1\n",
    "\n",
    "            else:\n",
    "                for j in range(19):\n",
    "                    dropped_row[str(j)] = \"UNENCOUNTERED FORMAT\"\n",
    "        else:\n",
    "            for j in range(19):\n",
    "                dropped_row[str(j)] = \"UNENCOUNTERED FORMAT\"\n",
    "    else:\n",
    "        dropped_row = {}\n",
    "\n",
    "    return dropped_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dropped_rows(source_directory, shock_list, s):\n",
    "    process_file = s[\"filename\"]\n",
    "    shock = s[\"shock\"]\n",
    "\n",
    "    check_dict = {}\n",
    "\n",
    "    file_path = os.path.join(source_directory, process_file)\n",
    "\n",
    "    # with statements automatically control the closing of files\n",
    "    with open(file_path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "        contents = contents.lower()\n",
    "\n",
    "        section_start = \"sección vii:\"\n",
    "        # empty because it is the last section\n",
    "        section_end = \"\"\n",
    "        section_search = find_between(section_start, section_end, contents)\n",
    "\n",
    "        dropped_row = get_dropped_row(section_search, shock, shock_list)\n",
    "\n",
    "        # check to make sure files contain the right amount of entries\n",
    "        if len(dropped_row) <= 1:\n",
    "            print(f\"Unencountered Format: {process_file}\\n\"\n",
    "                  + f\"Length of data: {len(dropped_row)}\\n\")\n",
    "\n",
    "        check_dict[\"filename\"] = process_file\n",
    "\n",
    "        for key, value in dropped_row.items():\n",
    "            check_dict[key] = value\n",
    "\n",
    "    # convert list to Series\n",
    "    check_series = pd.Series(data=check_dict)\n",
    "\n",
    "    return check_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = null_df.apply(lambda s: check_dropped_rows(source_directory,\n",
    "                                                      shock_list, s),\n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_check(dataf):\n",
    "    # fill missing values with null\n",
    "    dataf = dataf.replace(\"-\", np.NaN)\n",
    "    dataf = dataf.replace(\"\", np.NaN)\n",
    "\n",
    "    # drop rows which contain no information\n",
    "    isnull_sum = dataf.isnull().sum(axis=1)\n",
    "    drop_filter = isnull_sum >= 19\n",
    "    drop_indexes = dataf[drop_filter].index\n",
    "\n",
    "    dataf = dataf.drop(drop_indexes)\n",
    "\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df = (check_df\n",
    "               .pipe(start_pipeline)\n",
    "               .pipe(drop_null_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_null_df.shape[0] == 0:\n",
    "    datasets_directory = \"../datasets/\"\n",
    "    filename = \"section_7.csv\"\n",
    "    file_path = os.path.join(datasets_directory, filename)\n",
    "\n",
    "    clean_df.to_csv(file_path, index=False, na_rep=\"null\")\n",
    "\n",
    "    print(f\"Exported to {filename}\")\n",
    "else:\n",
    "    print(\"Error: Dropped rows contain non-null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Testing Code\n",
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Parsing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = clean_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\", \"shock_section\", \"shock\",\n",
    "                \"past_15_years_adversely_affected_by_change\",\n",
    "                \"most_important_change\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(clean_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dropped Rows\n",
    "Another way to check is by opening the variable inspector and manually scrolling\n",
    "through the `df_dropped` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tells us whether or not there are files that contain no information for this section. If raw file count is the same as clean file count, then no files were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw File Count: {}\".format(len(raw_df[\"filename\"].value_counts())))\n",
    "print(\"Clean File Count: {}\".format(len(clean_df[\"filename\"].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are files that are completely dropped, find those files and look at them to check for parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = raw_df[\"filename\"].value_counts().index\n",
    "clean_file_names = clean_df[\"filename\"].value_counts().index\n",
    "\n",
    "raw_file_names[~raw_file_names.isin(clean_file_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows value counts to see if any rows are being dropped that should not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = dropped_df.columns\n",
    "cols_to_drop = [\"filename\", \"interviewee\"]\n",
    "\n",
    "cols_to_check = cols_to_check.drop(cols_to_drop)\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"*\" * 50)\n",
    "    print(\" \" * 5 + col)\n",
    "    print(dropped_df[col].value_counts(dropna=False))\n",
    "    print(\"*\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = clean_df.columns[clean_df.columns != \"filename\"]\n",
    "clean_df[clean_df.loc[:, df_columns].duplicated(keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
